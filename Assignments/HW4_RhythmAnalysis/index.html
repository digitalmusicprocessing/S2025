<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 372: Digital Music Processing, Spring 2025</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 372: Digital Music Processing, Spring 2025</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Assignment 4: Tempo Estimation And Beat Tracking (50 Points)</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a>
									</header>

									<div id="page-content">

										<ul>
											<li><a href = "#overview">Overview/Logistics</a>
												<ul>
													<li><a href = "#objectives">Learning Objectives</a></li>
													<li><a href = "#submit">What To Submit</a></li>
												</ul>
											</li>
											<li><a href = "#programming">Programming Tasks</a>
												<ul>
													<li><a href = "#anf">Part 1a: Superflux Audio Novelty Functions (8 Points)</a></li>
													<li><a href = "#tempo">Part 1b Tempo Estimation</a>
														<ul>
															<li><a href = "#fouriertempo">Fourier-Based Tempo Estimation (7 Points)</li>
															<li><a href = "#dftacf">DFT-ACF Tempo Estimation (10 Points)</a></li>
														</ul>
													</li>
													<li>
														<a href = "#beat">Part 2: Beat Tracking</a>
														<ul>
															<li>
																<a href = "#sonifybeats">Sonifying Beats (5 Pts)</a>
															</li>
															<li>
																<a href = "#beattrack">Dynamic Programming Beat Tracking (20 Points)</a>
															</li>
														</ul>
													</li>
												</ul>
											</li>
											
										</ul>

										<h2><a name = "overview">Overview / Logistics</a></h2>

										<p>
											We are now moving into the analysis part of the course.  You will use mid-level features built on top of spectrograms both to estimate the overall tempo of a clip of audio, as well as getting the computer to "tap its virtual foot" to the beat.  Both the tempo and beat locations are aspects of rhythm.
										</p>
										

                                        <p>
                                            <h3><a name = "objectives">Learning Objectives</a></h3>
                                            <ul>
												<li>Practice numpy arrays, methods, and for loops in the service of musical applications</li>
												<li>Built audio novelty features on top of spectrograms and mel-spectrograms</li>
												<li>Use autocorrelation and the DFT to estimate tempo</li>
												<li>Implement a dynamic programming technique for beat tracking</li>
                                            </ul>
										</p>
										
										<h3><a name = "submit">What To Submit</a></h3>

										<p>                                       
                                            When you are finished, submit your python filed <code>novfn.py</code>, <code>tempo.py</code>, and <code>beat.py</code> to Canvas.
										


										
                                        
										<HR>
										<h2><a name = "programming">Programming Tasks</a></h2>
										<p>
											<a href = "https://github.com/digitalmusicprocessing/HW4_Rhythm/archive/refs/heads/main.zip">Click here</a> to download the starter code for this assignment.  This code includes two testing suites for tempo estimation and beat tracking from <a href = "https://www.music-ir.org/mirex/wiki/MIREX_HOME">MIREX</a>. Below are the imports you will need to test your code in jupyter:
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											%load_ext autoreload
											%autoreload 2
											import numpy as np
											import matplotlib.pyplot as plt
											import IPython.display as ipd
											from scipy.io import wavfile
											import pandas as pd
											from novfn import *
											from tempo import *
											from beat import *
										</script>

										<h2><a name = "anf">Part 1a: Superflux Audio Novelty Functions (8 Points)</a></h2>
										<p>
											There is a whole zoo of possible ways of computing audio novelty functions that pick up on rhythmic events.  For example, even all the way back in 2007, the authors of <a href = "http://www.cp.jku.at/research/papers/gouyon_etal_icassp_2007.pdf">this paper</a> experimented with 172 unique such novelty functions!   In this section, we will explore a more recent method known as <b>superflux</b> for generating superior audio functions to the versions we described in <a href = "../../../Modules/Module16/Video1">module 16</a>.  This technique is described in detail in <a href = "http://phenicx.upf.edu/system/files/publications/Boeck_DAFx-13.pdf">this paper</a> from 2013, but we will summarize the key steps below.  Before we do that, though, let's look at an example of how superflux compares to the naive technique.  Consider the following audio clip, obtained from the <a href = "https://github.com/librosa/librosa_gallery/blob/master/audio/Karissa_Hobbs_-_09_-_Lets_Go_Fishin.mp3">librosa gallery</a>.
										</p>

										<p>
											<audio controls>
												<source src="HW4_Rhythm/fishin.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<p>
											Then, once you've implemented superflux by filling in the method <code>get_superflux_novfn</code>, the following code will run.
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr, x = wavfile.read("fishin.wav")
											x = x/32768.0
											hop_length = 256
											win_length = 2048
											mu = 8
											y1 = get_novfn(x, sr, hop_length, win_length)
											y2 = get_superflux_novfn(x, sr, hop_length, win_length, mu=mu, Gamma=1, max_win=3)
											 
											plt.figure(figsize=(10, 6))
											plt.subplot(211)
											plt.plot(np.arange(len(y1))*hop_length/sr, y1)
											plt.title("Fishin' Naive Novelty Function")
											plt.xlabel("Time (Sec)")
											 
											plt.subplot(212)
											plt.plot(np.arange(len(y2))*hop_length/sr, y2)
											plt.title("Fishin' Superflux Novelty Function")
											plt.tight_layout()
										</script>

										<p>
											And produce the figure below:
										</p>

										<p>
											<img src = "Fishin.svg" width="90%">
										</p>

										<p>
											As you can see, the peaks in the audio novelty function are better and more clearly separated from the background noise.  We can also hear the difference between the two if we apply the provided sonification method that shapes noise, as explained in module 16
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											s1 = sonify_novfn(y1, hop_length)
											ipd.Audio(s1, rate=sr)
										</script>

										<p>
											<audio controls>
												<source src="sonifynaive.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<p>
											Notice how much clearer the rhythmic events sound in the superflux version
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											s2 = sonify_novfn(y2, hop_length)
											ipd.Audio(s2, rate=sr)
										</script>

										<p>
											<audio controls>
												<source src="sonifysuperflux.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<h3><a name = "superfluxalgorithm">Your Task: Superflux Algorithm Steps</a></h3>

										<p>
											Fill in the method <code>get_superflux_novfn</code> in the file <code>novfn.py</code> to implement the superflux pipeline, performing the following steps in sequence:
										</p>

										<ol>
											<li>
												Compute the absolute magnitude spectrogram (this has been provided for you)
											</li>
											<li>
												Convert the spectrogram to a <b><a href = "../../../Modules/Module16/Video4">mel spectrogram</a></b>.  As described in Section 2.1 of the <a href = "http://dafx13.nuim.ie/papers/09.dafx2013_submission_12.pdf">superflux paper</a>, the mel filterbank should contain 138 bins spanning frequencies from <b>27.5 hz</b> to <b>16000hz</b>. 
											</li>
											<li>
												Convert every element of the magnitude Mel-spectrogram <b>MS</b> to a log magnitude scale by applying the equation <b>log<SUB>10</SUB>(MS + Gamma)</b>, where <b>Gamma</b> is a provided parameter (set to 1 in the <a href = "http://phenicx.upf.edu/system/files/publications/Boeck_DAFx-13.pdf">superflux paper</a>, as shown in equation 4 on page 3).
											</li>
											<li>
												Perform a <b>maximum filter</b>; that is, replace every amplitude in every window by the maximum of the amplitudes of a window of length <code>max_win</code> around them.  You can use <a href = "https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.maximum_filter.html">scipy's <code>maximum_filter</code></a> method, which has already been imported as <code>maximum_filter</code>, to do this in one line.  The shape of the filter should be <code>(max_win, 1)</code>, or, in other words, a vertical window of <code>max_win in size</code>.
											</li>
											<li>
												Take the sum of <b>positive differences of amplitudes</b> across all frequency bins from a window to a window in the future, similarly to the naive version (recall that in the positive difference, we don't add things to the sum that are negative, so this is not the same as the absolute value).  However, the hop length in superflux is generally taken to be smaller than usual so the windows are sampled at a higher resolution, leading to a higher resolution audio novelty function.  This means that adjacent windows change less.  To compensate for this, instead of taking the difference in amplitudes for corresponding frequencies in adjacent windows, you should take differences with windows that are <code>mu</code> hops ahead in the future (as described in Section 2.1 of <a href = "http://phenicx.upf.edu/system/files/publications/Boeck_DAFx-13.pdf">the superflux paper</a>).

												<p>
													<b>Hint:</b> Be sure to review <a href = "../../../Modules/Module16/Video0">module 16 video 0 and video 1</a> to recall how to do the positive difference step.
												</p>
											</li>
										</ol>


										<h2><a name = "tempo">Part 1b Tempo Estimation</a></h2>
										<p>
											Now that we have clean audio novelty functions, we can estimate tempo from them with various techniques.  To evaluate the results, we will compute tempo estimates with our code on the 20 examples in the tempo dataset.  Provided with the tempo dataset are two tempos: a slow tempo and a fast tempo.  We declare that our estimated tempo is correct if it is within 8% of either of these two "ground truth" tempos.
										</p>
										
										<h3>General Tip</h3>
										<p>
											I have provided code to test these methods on the dataset of 20 clips, but I would recommend just throwing some plots into jupyter as you incrementally develop these methods until you're confident you know what's going on.  You don't even need to write code in tempo.py right away; just see if you can get an example to work in an notebook cell.  Then, you can write a more general purpose method, and you can test and see if you get similar results.
										</p>

										<h3><a name = "fouriertempo">Fourier-Based Tempo Estimation (7 Points)</a></h3>

										<h4>Your Task</h4>
										<p>
											Fill in the method <code>get_fourier_tempo</code> to estimate the tempo, in beats per minute, of a tune based on its audio novelty function <b>after subtracting its mean</b> (this makes it so that bin 0, the average, is 0 and doesn't throw off the estimate).
										</p>

										<h4>Description</h4>
										<p>
											Since rhythm is repetitive in popular music genres, the audio novelty function should be periodic; i.e. the spacing between beats should be fairly consistent, up to some missing beats for syncopation and other effects.  This means that the Discrete Fourier Transform should have peaks at frequencies corresponding to the tempo, and possibly its harmonics.  Let's consider the superflux novelty function of <code>train4.wav</code> in the tempo dataset (a clip from <a href = "https://www.youtube.com/watch?v=RPFyeCmHXjY">"Green Eyes"</a> by Erykah Badu), as obtained by the following code
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr, x = wavfile.read("Testing/Audio/train4.wav")
											hop_length = 512
											win_length = 2048
											max_win = 3
											mu = 4
											Gamma = 1
											novfn = get_superflux_novfn(x, sr, hop_length, win_length, max_win, mu, Gamma)
											novfn = novfn - np.mean(novfn) # Mean-center
										</script>

										<p>
											If we plot the magnitudes of the first 400 frequency bins of the DFT, we see the following plot:
										</p>

										<img src = "EBDFT.svg" width="80%">

										<p>
											There are three peaks at the beginning, and we end up finding that the max occurs at the third such bin at frequency index 63.  The sample rate is 44100 and the hop length is 256, which means we have <b>(44100/256)</b> samples of the audio novelty function per second.  The audio clip is 30 seconds long, which translates into an interval of <b>5165</b> samples in the audio novelty function at this rate.  We put this all together to find that the frequency is 
										</p>
										
										<h4>
											\[ \left( \frac{63 \text{ cycles}}{5165 \text{ samples}} \right)  \left( \frac{44100/256 \text{ samples}}{1 \text{ second}} \right)  \left(  \frac{60 \text{ seconds}}{1 \text{ minute}}  \right) = 126 \text{ beats/minute} \]
										</h4>

										<p>
											Actually, if we look at the ground truth tempos in the dataset, we see that they are 42bpm and 126bpm.  So we actually found the higher tempo here.  This tune is counted in triplets, so the higher tempo is 3x the lower tempo.  We also see the lower tempo show up as a peak at index 21, which is 1/3 of the index we chose, but it is not the max peak.
										</p>
										

										<h4>Hint</h4>
										<p>
											Be careful that you only consider the first half of the DFT!  Since a DFT with <b>N</b> frequencies on <b>N</b> samples is redundant, the second half will be the mirror image of the first half.  If you pick a frequency in the second half, you will get what appears to be a ridiculously high tempo, but this is actually just the mirror of a much lower tempo.
										</p>

										<h4>Results</h4>
										<p>
											Let's first run this technique using the vanilla audio novelty function on all 20 clips.  As we see, we get only <b>6/20</b> to within 8% of at least one of the two ground truth tempos.  However, we see that many of the ones that are marked wrong are harmonics of the true tempo.  For instance, we report a tempo of 306 for train3, which is double the highest tempo.  This is one of the downsides of using Fourier here; the impulsive beats have many harmonics in their Fourier representation, and we sometimes pick a harmonic instead of the true base frequency corresponding to the rhythm.  We will try to address this with another technique later, but it is a problem that will plague any tempo estimator to some extent.
										</p>

										<iframe src="vanilla_acf_dft.html" width="800" height="500"></iframe>

										<p>
											One thing we can change here is to use an improved audio novelty function.  So let's swap in the superflux method we worked so hard for.  We get <b>8/20</b> here, so better, but probably not a statistically significant improvement
										</p>

										<iframe src="superflux_fourier.html" width="800" height="500"></iframe>
										<p>

										</p>


										<h3><a name = "dftacf">DFT-ACF Tempo Estimation (10 Points)</a></h3>

										<h4>Your Task</h4>
										<p>
											Fill in the method <code>get_acf_dft_tempo</code> in <code>tempo.py</code> to compute an improved tempo estimate, based on a combination of the autocorrelation function (aka ACF, as described in <a href = "../../../Modules/Module16/Video2">module 16</a>) and the DFT .
										</p>

										<h4>
											Description
										</h4>
										<p>
											The algorithm is described in <a href = "https://link.springer.com/content/pdf/10.1155/2007/67215.pdf">Section 3.1.1 of Peeters, 2007</a>, but we will summarize the theory and the steps below.
										</p>
											As we discussed, one of the issues with using the DFT is that we sometimes pick harmonics of the true rhythm, since harmonics exist for any periodic signal that isn't a pure sinusoid.  We could try the autocorrelation function (ACF), but, as it turns out, this has the opposite problem: <i>subharmonics</i> of tempo.  The ACF takes a dot product between a signal and its shifts, so it will have a local max if the shift coincides with the period <b>T</b>.  But this also means that it will have peaks at <b>2T</b>, <b>3T</b>, <b>4T</b>, etc.  Since the the period is inversely related to the frequency, these longer periods actually correspond to fractions of tempo.
										</p>

										<p>
											We can try to get the best of both worlds by multiplying DFT by the ACF at each possible tempo.  The DFT should be small at subharmonics, while the ACF should be small at harmonics, so all but the true tempo should be damped down.  The challenge here is that the ACF and DFT are expressed in different units.  In particular
											<ul>
												<li>The DFT is expressed as a <b>frequency</b> as a number of cycles over the extent of the audio novelty function.</li>
												<li>The ACF is expressed as a <b>period</b> in samples per cycle.</li>
											</ul>
										</p>
										<p>
											What this means is that we have to <i>warp</i> the domain of the DFT to coincide with the domain of the ACF.  In this process, higher bins of the DFT should turn into lower bins of the ACF, since period and frequency are inversely related.  Visually, this looks like taking the mirror image of the DFT and compressing it towards the origin.  Mathematically, for an audio novelty function with <b>N</b> samples, a shift <b>T</b> in the ACF corresponds to frequency index <b>N/T</b>.  So we want to create the warped dft, dftw, as the composition 
										</p>
										
										<h3>
											\[ \text{dftw}[T] = \text{dft}[N/T] \]
										</h3>
											
										<p>
											But note that <b>N/T</b> may not be an integer, so to figure out what should go in warped DFT bin <b>T</b>, we should <b>interpolate</b> between the floor and the ceiling of <b>N/T</b>.  We can do this using <a href = "https://en.wikipedia.org/wiki/Linear_interpolation">linear interpolation</a>.  Let <b>i1</b> be <b>int(np.floor(N/T))</b> and <b>i2</b> be <b>int(np.ceil(N/T))</b>, then the linear interpolation is 
										</p>

										<h3>
											\[ \text{dftw}[T] = \left( \frac{N}{T}-i1 \right)*\text{dft}[i2] + \left( i2-\frac{N}{T} \right)*\text{dft}[i1] \]
										</h3>

										<h4>Example: <code>train5.wav</code></h4>
										<p>
											The image below shows this on <code>train5.wav</code> in the tempo dataset.  As you can see, the ACF (first plot) has many peaks at integer multiples of the first peak at index 76, while the DFT (second plot) has a peak at the first harmonic of its maximum at index 102.  If we warp the DFT (third plot) by takings its mirror image and compressing it towards the origin, then it lines up with the ACF, and we can take the point by point product, which gives us the combined plot on the bottom with the harmonics and subharmonics cancelled out.  We can now say confidently that the tempo occurs at a shift of 76 samples.  This occurred at a hop length of 512 samples at a sample rate of 44100.  The general equation for converting a shift <b>idx</b> into beats per minute (bpm) is:
										</p>

										<h3>
											\[ \left(  \frac{1}{\text{idx}} \frac{\text{cycle}}{\text{shift}} \right) \left(  \frac{1}{\text{hop}} \frac{\text{shift}}{\text{samples}} \right) \left(  \text{sr} \frac{\text{samples}}{\text{sec}} \right)  \left( 60 \frac{\text{sec}}{\text{minute}} \right) \]
										</h3>

										<p>
											Or, in other words, <b>60*sr / (idx*hop)</b>. 
											For <b>idx = 76, sr=44100, hop=512</b>, this is 68 bpm, which is quite close to one of the ground truth tempos of 68.5 bpm.
										</p>
										
										<p>
											<img src = "DFT_ACF_5.svg" width="90%">
										</p>

										<h4>Example: <code>train14.wav</code></h4>

										<p>
											Below is another example on <code>train14.wav</code> in the tempo dataset.  Here we really see the effect of damping down the subharmonics of the ACF
										</p>
										<p>
											<img src = "DFT_ACF_14.svg" width="90%">
										</p>

										<h4>Results</h4>
										<p>
											Below are the results, which show a substantial improvement over everything we've seen so far, with a <b>10/20</b>!  Obviously, there is still room for improvement, but this is a surprisingly good agreement with human annotations given how little code we wrote, and most of the errors are "octave errors" where the tempo is a double or a half of one of the ground truth tempos.
										</p>

										<iframe src="superflux_acf_dft.html" width="800" height="500"></iframe>

										<p></p><p></p>



										<h2><a name = "beat">Part 2: Beat Tracking</a></h2>					
										
										<p>
											In this section, you will implement the dynamic programming beat tracking algorithm described at <a href = "../../ClassExercises/BeattrackNotes/">this link</a>, editing code in the <code>beat.py</code> file.  
										</p>

										<p>
											<b>NOTE: </b> This method of tracking beats requires us to have an estimate of the tempo up front.  Of course, we could use the methods from <a href = "#tempo">the last part</a> to compute this before running beat tracking.  
										</p>
										
										<p><b>NOTE ALSO: </b>If you're curious about other techniques for beat tracking in python, check out <a href = "https://madmom.readthedocs.io/en/latest/">madmom</a>.
										</p>

										<h3><a name = "sonifybeats">Sonifying Beats (5 Pts)</a></h3>
										<p>
											As with all of our features, it's helpful to "sonify" what we have computed so we can hear its quality.  So let's setup these tools before we even attempt to track the beats
										</p>
										<h4>Your Task</h4>
										<p>
											
											Fill in the method <code>sonify_beats</code> to create little blips that occur at beat locations.  In particular, you should sonify each beat by adding in a small 440hz sinusoid at each beat location.  Each blip should be added in to a slice of the array starting at the beat location and continuing for <code>blip_len</code> seconds.  For example, if you run the following code to load one of the ground truth human annotators on the third train clip
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr = 44100
											beats = get_gt_beats("Testing/Beattrack/train4.txt", 3) # Load in ground truth beats for annotator 3
											sr, x = wavfile.read("Testing/Audio/train4.wav")
											x = x/32768.0 # Put into the range [-1, 1]
											y = sonify_beats(x, sr, beats)
											ipd.Audio(y, rate=sr)
										</script>
										<p>
											then you should hear this
										</p>
										<audio controls>
											<source src="Examples/beatsonify1.mp3" type="audio/mpeg">
										  Your browser does not support the audio element.
										</audio> 

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr = 44100
											beats = get_gt_beats("Testing/Beattrack/train1.txt", 0) # Load in ground truth beats for annotator 0
											sr, x = wavfile.read("Testing/Audio/train1.wav")
											x = x/32768.0 # Put into the range [-1, 1]
											y = sonify_beats(x, sr, beats)
											ipd.Audio(y, rate=sr)
										</script>
										<p>
											then you should hear this
										</p>
										<audio controls>
											<source src="Examples/beatsonify2.mp3" type="audio/mpeg">
											Your browser does not support the audio element.
										</audio> 
										<P></P>

										<h3><a name = "beattrack">Dynamic Programming Beat Tracking (20 Points)</a></h3>
										
										<h4>Your Task</h4>
										<p>
											Fill in the method <code>get_beats</code> to perform <a href = "../../ClassExercises/BeattrackNotes/">dynamic programming beat tracking</a>.  You can use both the <code>sonify_beats</code> method, as well as the provided <code>plot_beats</code> method, to help check to make sure it's working well.  For example, consider generating the following novelty function for the first example in the beats dataset
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											hop_length = 256
											win_length = 2048
											max_win = 6
											mu = 1
											Gamma = 1
											sr, x = wavfile.read("Testing/Audio/train1.wav")
											x = x/32768.0
											novfn = get_superflux_novfn(x, sr, hop_length, win_length, max_win, mu, Gamma)
											</script>

										<p>
											The tempo in this tune is 129.5 beats per minute.  If we're slightly off on our tempo estimate and guess 120, and we're also very permissive with sticking to the tempo, we don't always get a consistent pulse, and we get subdivided beats at times
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											beats = get_beats(novfn, sr, hop_length, 120, 1)
											plt.figure(figsize=(12, 5))
											plot_beats(novfn, beats, sr, hop_length)
											plt.xlim([0, 10])
											y = sonify_beats(x, sr, beats)
											ipd.Audio(y, rate=sr)
											</script>
										<audio controls>
											<source src="Examples/Beat_1_120_1.mp3" type="audio/mpeg">
											Your browser does not support the audio element.
										</audio> 
										<p>
											<img src = "Examples/Beat_1_120_1.svg" width="80%">
										</p>
										<p>
											If, however, we increase the &alpha; penalty of tempo deviation from 1 to 100, the algorithm is less tempted to add beats on some of the high audio novelty function regions in subdivided places
										</p>
										<audio controls>
											<source src="Examples/Beat_1_120_100.mp3" type="audio/mpeg">
											Your browser does not support the audio element.
										</audio> 
										<p>
											<img src = "Examples/Beat_1_120_100.svg" width="80%">
										</p>

										<p>
											Here's a slightly tougher example
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											hop_length = 256
											win_length = 2048
											max_win = 6
											mu = 4
											Gamma = 1
											sr, x = wavfile.read("Testing/Audio/train2.wav")
											x = x/32768.0
											novfn = get_superflux_novfn(x, sr, hop_length, win_length, max_win, mu, Gamma)

											beats = get_beats(novfn, sr, hop_length, 167, 200)
											plt.figure(figsize=(12, 5))
											plot_beats(novfn, beats, sr, hop_length)
											plt.xlim([0, 10])
											y = sonify_beats(x, sr, beats)
											plt.savefig("Beat_1_120_100.svg", bbox_inches='tight')
											ipd.Audio(y, rate=sr)
											</script>
										
										<p>
											And here are the results
										</p>
										<audio controls>
											<source src="Examples/train2beattrack.mp3" type="audio/mpeg">
											Your browser does not support the audio element.
										</audio> 
										<p>
											<img src = "Examples/Train2Beattrack.svg" width="80%">
										</p>

										<p>
											Here's yet another example, where we really have to turn up &alpha; so that the algorithm is not tempted to take a few large peaks in novelty to stay on tempo (e.g. around 3 seconds and 5.5 seconds)
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											hop_length = 256
											win_length = 2048
											max_win = 6
											mu = 4
											Gamma = 1
											sr, x = wavfile.read("Testing/Audio/train3.wav")
											x = x/32768.0
											novfn = get_superflux_novfn(x, sr, hop_length, win_length, max_win, mu, Gamma)

											beats = get_beats(novfn, sr, hop_length, 153, 200)
											plt.figure(figsize=(12, 5))
											plot_beats(novfn, beats, sr, hop_length)
											plt.xlim([0, 10])
											y = sonify_beats(x, sr, beats)
											plt.savefig("Train2_Beattrack.svg", bbox_inches='tight')
											ipd.Audio(y, rate=sr)

										</script>

										<p>
											And here are the results
										</p>
										<audio controls>
											<source src="Examples/train3beattrack.mp3" type="audio/mpeg">
											Your browser does not support the audio element.
										</audio> 
										<p>
											<img src = "Examples/Train3Beattrack.svg" width="80%">
										</p>

										
										
                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#homework">Homework</a></li>
												<li><a href = "../../index.html#grading">Grading</a>
													<ul>
														<li><a href = "../../index.html#deadlines">Deadlines Policy</a></li>
													</ul>
												</li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#participation">Participation</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../../webaudio-pianoroll/index.html">Piano Roll Editor</a></li>
										<li><a href = "https://calebj0seph.github.io/spectro/">Live Spectrogram Viewer</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../Assignments/HW1_RissetBeats">HW1: Risset Beats</a>
													<ul>
														<li>
															<a href = "../../Assignments/HW1_RissetBeats/statements.html">Musical Statements</a>
														</li>
													</ul>
												</li>
												<li><a href = "../../Assignments/HW2_DigitalInstruments">HW2: Digital Instruments</a>
												<ul>
													<li>
														<a href = "../../Assignments/HW2_DigitalInstruments/statements.html">Musical Statements</a>
													</li>
												</ul>
												</li>
												<li><a href = "../../Assignments/HW3_Vocoders">HW3: Spectacular Spectrograms</a>
												<!--<ul>
													<li>
														<a href = "../../Assignments/HW3_Vocoders/statements.html">Musical Statements</a>
													</li>
												</ul>!-->
												</li>
												<li><a href = "../../Assignments/HW4_RhythmAnalysis">HW4: Tempo Estimation And Beat Tracking</a></li>
												<!--
												<li><a href = "../../Assignments/HW5_LetItBee">HW5: Let It Bee</a>
												<ul>
													<li>
														<a href = "../../Assignments/HW5_LetItBee/statements.html">Musical Statements</a>
													</li>
												</ul>
												</li>
												<li><a href = "../../Assignments/HW6_StringAlong">HW6: String Along</a>
												<ul>
													<li>
														<a href = "../../Assignments/HW6_StringAlong/statements.html">Musical Statements</a>
													</li>
												</ul>
												
												</li>
												-->
											</ul>
										</li>
										<li>
											<span class="opener">Class Exercises</span>
											<ul>
												<li><a href = "../../ClassExercises/Week1/Week1_AudioReverseGame/">Week 1: Audio Reverse Game</a></li>
												<li><a href = "../../ClassExercises/Week2/Week2_BeatPhase/index.html">Week 2: Beat Phase</a></li>
												<li><a href = "../../ClassExercises/RissetNotes/index.html">Week 2: Notes on Risset Beats</a></li>
												<li><a href = "../../ClassExercises/Week2/Week2_Harmonicity/index.html">Week 2: Harmonicity</a></li>
												<li><a href = "../../ClassExercises/Week3/Week3_ZCS_Loudness/index.html">Week 3: Zero Crossings And Loudness Perception</a></li>
												<li><a href = "../../ClassExercises/Week3/Week3_Timbre/index.html">Week 3: Harmonics And Timbre</a></li>
												<li><a href = "../../ClassExercises/Week4/Week4_Envelopes/index.html">Week 4: Timbral Envelopes</a></li>
												<li>
													<a href = "../../ClassExercises/Week4/Week4_CombFilters/index.html">Week 4: Comb Filters</a>
												</li>
												<li><a href = "../../ClassExercises/Week5/Week5_DFT/index.html">Week 5: The Discrete Fourier Transform</a></li>
												<li><a href = "../../ClassExercises/Week6/2DArrays/index.html">Week 6: 2D Arrays And Spectrograms</a></li>
												<li><a href = "../../ClassExercises/Week6/ComplexDFT/index.html">Week 6: Complex DFT</a></li>
												<li><a href = "../../ClassExercises/Week7/Week7_STFTNoiseShaping">Week 7: STFT Noise Shaping</a></li>
												<li><a href = "../../ClassExercises/Week8/Week8_ANF">Week 8: Audio Novelty Functions</a></li>
												<li><a href = "../../ClassExercises/BeattrackNotes">Week 9: Notes on Dynamic Programming Beat Tracking</a></li>
												<li><a href = "../../ClassExercises/Week9/Week9_MFCC">Week 9: Mel-Frequency Cepstral Coefficients (MFCCs)</a></li>
												<li><a href = "https://github.com/ursinus-cs372-s2023/pyshazam">Week 9: Python Implementation of Shazam</a></li>
												<li><a href = "https://github.com/ursinus-cs372-s2023/Week10_HPSS/tree/classcode">Week 10: Harmonic/Percussive Source Separation with Median Filters</a></li>
												<li><a href = "../../ClassExercises/Week10/Week10_NMF">Week 10: Nonnegative Matrix Factorization for Demixing</a></li>
												<li><a href = "../../ClassExercises/Week11/">Week 11: Fundamental Frequency Tracking And Autotuners</a></li>
												<li><a href = "../../../Modules/Module21/Video0">Week 12: Linear Separability of Phase-Shifted Triangle/Square Waves</a></li>
											</ul>
										</li>
                                        <li>
											<span class="opener">Pre-Class Modules</span>
											<ul>
												<li><a href = "../../../Modules/Module1/Video0">Module 1: Digital Audio Waveforms, Python Basics</a></a></li>
												<li><a href = "../../../Modules/Module2/Video1">Module 2: Sinusoids And Simple Numpy Tunes</a></li>
												<li><a href = "../../../Modules/Module3/Video0">Module 3: Standing Waves And Plucked String Synthesis</a></li>
												<li><a href = "../../../Modules/Module4/Video1">Module 4: Chirps, Instantaneous Frequency, Vibrato, Sonification</a></li>
												<li><a href = "../../../Modules/Module5/Video1">Module 5: Zero Crossings Filtering, Loudness And Intensity / Dynamics</a></li>
												<li><a href = "../../../Modules/Module6/Video0">Module 6: Timbre, FM Synthesis, Python Methods As Parameters</a></li>
												<li><a href = "../../../Modules/Module7/Video1">Module 7: Echoes, Impulse Responses, And Convolution</a></li>
												<li><a href = "../../../Modules/Module8/Video1">Module 8: Discovering The Discrete Fourier Transform</a></li>
												<li><a href = "../../../Modules/Module9/Video0">Module 9: The Real Discrete Fourier Transform (DFT), Amplitude/Phase</a></li>
												<li><a href = "../../../Modules/Module10/Video1">Module 10: DFT on Real Audio, DFT on Sawtooth/Square Waves, Fundamental DFT Properties, Inverse DFT And Fast Risset</a></li>
												<li><a href = "../../../Modules/Module11/Video1">Module 11: STFT, Window Functions, Complex Numbers</a></li>
												<li><a href = "../../../Modules/Module12/Video1">Module 12: Complex DFT And Phasors</a></li>
												<li><a href = "../../../Modules/Module13/Video1.html">Module 13: Aliasing, Inverse DFT</a></li>
												<li><a href = "../../../Modules/Module14/Video1">Module 14: Convolution And Multiplication Duality</a></li>
												<li><a href = "../../../Modules/Module15/Video0">Module 15: The Z Transform And Linear Predictive Coding</a></li>
												<li><a href = "../../../Modules/Module16/Video0">Module 16: Audio Novelty Functions, Tempo Estimation, Matrix Multiplication</a></li>
												<li><a href = "../../../Modules/Module17/Video0">Module 17: Sonifying Mel And Chroma Filterbanks</a></li>
												<li><a href = "../../../Modules/Module18/Video0">Module 18: Matrix Multiplication for Audio Activations</a></li>
												<li><a href = "../../../Modules/Module19/Video0">Module 19: Self-Similarity Matrices</a></li>
												<li><a href = "../../../Modules/Module20/Video1">Module 20: Intro To Supervised Learning, Logistic Regression, Gradient Descent, And PyTorch</a></li>
												<li><a href = "../../../Modules/Module21/Video0">Module 21: Neural Networks</a></li>
												<li><a href = "../../../Modules/Module22/Video1">Module 22: Multiclass Classification, Convolutional Neural Networks, And Overfitting</a></li>
											</ul>
										</li>
										<li><a href = "https://docs.google.com/forms/d/e/1FAIpQLSfwkO_w_Ku-n2Ou6J7pF--i0C2-a20Ov9wf690T6cYx80ASsw/viewform?usp=sf_link">Anonymous Question</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<div class="mini-posts">
										Announcements							
                                    </div>
								</section>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
	</body>