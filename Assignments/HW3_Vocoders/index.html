<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- Header !-->
	<head>
		<title>Ursinus CS 372: Digital Music Processing, Spring 2025</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="../../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
        <style>
        .image_off, #home:hover .image_on{
           display:none
        }
        .image_on, #home:hover .image_off{
           display:block
        }
        </style>
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="../../index.html" class="logo"><strong>Ursinus CS 372: Digital Music Processing, Spring 2025</strong></a>
								</header>
<!-- End Header !-->

							<!-- Content -->
								<section>
									<header class="main">
                                        <h2>Assignment 3: Spectacular Spectrograms (50 Points)</h2>
                                        <h3><a href = "http://www.ctralie.com">Chris Tralie</a></h3>
										<h3><a href = "statements.html">Click here</a> to view musical statements</h3>
									</header>

									<div id="page-content">

										<ul>
											<li><a href = "#overview">Overview/Logistics</a>
												<ul>
													<li><a href = "#objectives">Learning Objectives</a></li>
													<li><a href = "#submit">What To Submit</a></li>
												</ul>
											</li>
											<li><a href = "#programming">Programming Tasks</a>
												<ul>
													<li>
														<a href = "#comb">Part 1: Comb Filter Vocoder (10 Points)</a>
													</li>
													<li>
														<a href = "#part2">Part 2: STFT-Based Vocoder</a>
														<ul>
															<li><a href = "#istft">Inverse STFT (8 Points)</a></li>
															<li><a href = "#stftvocoder">STFT-Based Vocoder (7 Points)</a></li>
														</ul>
													</li>
													<li>
														<a href = "#part3">Part 3: Phase Retrieval</a>
														<ul>
															<li><a href = "#griffinlim">The Griffin-Lim Algorithm (8 Points)</a></li>
															<li><a href = "#soundimages">Phase Retrieval Application 1: Sound Images (7 Points)</a></li>
															<li><a href = "#beepytunes">Phase Retrieval Application 2: Beepy Tunes, Aka Tricking Shazam (7 Points)</a></li>
															<li><a href = "#pitchshift">Phase Retrieval Application 3: Pitch Shifting (Extra Credit +3)</a></li>
														</ul>
													</li>
													
													
													<li>
														<a href = "#musical">Part 4: Musical Statement (3 Points)</a>
													</li>
													<li>
														<a href = "#bored">For the bored...</a>
													</li>
												</ul>
											</li>
											
										</ul>

										<h2><a name = "overview">Overview / Logistics</a></h2>

										<p>
											Can't sing?  No problem.  In this assignment, you will implement several algorithms to create "singing instruments," which is an application of something known as a <b>vocoder</b>.  Students will also implement a procedure known as "phase retrieval" to estimate phase from amplitude-only information.  Both of these tasks will be a great opportunity to practice Short-Time Fourier Transform concepts in the service of a slew of fun applications.  More background information and directions will be interspersed in the <a href = "#programming">programming task descriptions</a>.
										</p>
										

                                        <p>
                                            <h3><a name = "objectives">Learning Objectives</a></h3>
                                            <ul>
												<li>Practice numpy arrays, methods, and for loops in the service of musical applications</li>
												<li>Extract and manipulate amplitude and phase from Short-Time Fourier Transform coefficients</li>
												<li>Line up audio samples in arrays properly using slices</li>
												<li>Convert seamlessly between time and frequency representations of audio</li>
                                            </ul>
										</p>
										
										<h3><a name = "submit">What To Submit</a></h3>

										<p>                                       
                                            This assignment is split into two different deadlines. For each deadline, submit your python file <code>vocoder.py</code> to canvas.  The deadlines are as follows:
											<ul>
												<li><b>Submission 1: </b>
												<a href = "#comb">Comb Filter Vocoder</a>,
												<a href = "#istft">Inverse STFT</a>,
												<a href = "#stftvocoder">STFT-Based Vocoder</a>
												</li>
												<li><b>Submission 2: </b>
												<a href = "#griffinlim">The Griffin-Lim Algorithm</a>,
												<a href = "#soundimages">Sound Images</a>,
												<a href = "#beepytunes">Beepy Tunes</a>,
												<a href = "#musical">Musical Statement</a>
												</li>
											</ul>
											
										</p>
										<p>
											
											For the second submission, also submit an audio file for your musical statement and all of the txt files, audio files, and code that is needed to run your code used to create that statement.  Finally, please submit answers to the following questions on Canvas
										
										<ol>
											<li>
												A title for your musical statement
											</li>
											<li>
												If you want to submit your musical statement to the music contest, and if so, what name or pseudonym you would like to use in the musical gallery on our class web site
											</li>
											
										</ol>


										
                                        
										<HR>
										<h2><a name = "programming">Programming Tasks</a></h2>
										<p>
											<a href = "https://github.com/ursinus-cs372-s2023/HW3_Vocoders/archive/refs/heads/main.zip">Click here</a> to download the starter code for this assignment.  In all of the tasks below, you will be editing methods in the <code>vocoder.py</code> file.  You should also copy your <code>instruments.py</code> file from <a href = "../HW2_DigitalInstruments/">homework 2</a> into this directory, as it will be used to generate instruments in part 2 that will be shaped by voice.
										</p>

										<p>
											<b>Tip:</b> As a general tip for all of the tasks, you can save yourself <i>a lot</i> of code by using element-wise operations in python.  For instance, if <b>S</b> is an <b>M x N</b> matrix holding the complex STFT of some signal, you can compute the amplitude of every single sinusoid in every single window simply by saying
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											amps = np.abs(S)
										</script>

										<p>
											Similarly, you can compute all of the phases by saying 
										</p>
										

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											phases = np.arctan2(np.imag(S), np.real(S))
										</script>

										<p>
											Don't forget about your element-wise multiplication operations either!
										</p>

										<p>
											Below are the imports you will need in jupyter
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											%load_ext autoreload
											%autoreload 2
											import numpy as np
											import matplotlib.pyplot as plt
											from scipy.io import wavfile
											from scipy.signal import fftconvolve
											from scipy.ndimage import maximum_filter, maximum_filter1d
											from scipy.interpolate import interp2d
											import IPython.display as ipd
											from vocoder import *
											from instruments import make_tune, fm_bell_note
										</script>

										<h2><a name = "comb">Part 1: Comb Filter Vocoder (10 Points)</a></h2>
										<p>
											We saw in a <a href = "../../ClassExercises/Week4/Week4_CombFilters/solutions.html">class exercise</a> that it is possible to make a pitched "robot sound" by applying <a href = "https://ursinus-cs472a-s2021.github.io/Modules/Module7/Video1">convolution</a> to a sound with a comb filter.  In order to control the pitch, we noted that the spacing between impulses on the comb needs to be equal to the period, in samples, of the base frequency of the note we seek.  You will use this observation to make an entire tune of such notes by applying different comb filters to different slices of audio, with one slice for each note.  
										</p>
										<h4>Your Task</h4>
										<p>
											Fill in the method <code>comb_tune</code>.  For each note, you should create a comb filter with <code>num_pulses</code> "teeth" with a spacing appropriate for that note.  You should then convole that comb with the chunk of audio, referred to as <code>xi</code> in the code, using <code>fftconvolve</code>.  
										</p>

										<p>
											One thing to watch out for is if you perform convolution between two arrays of length <b>M</b> and <b>N</b>, the result will actually be of length <b>M + N-1</b>, because the echoes need some time to die out (you can verify the length of the convolution using <code>len()</code>).  So you will have to add the audio samples into the output <code>y</code>, and you will also have to be careful not to go out of bounds on the last window.  The picture below shows this overlap.
										</p>

										<p>
											<img src = "CombConvolution.svg" width = 300>
										</p>

										<p>
											Here are a few examples of results you should get if this is working properly
										</p>

										<h3>Happy Birthday Dear Christopher</h3>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											y, sr = comb_tune("Tunes/birthday.txt", "birthday.wav", 0.125, 20)
											ipd.Audio(y, rate=sr)
										</script>
										<h4>Voice</h4>
										<p>
											<audio controls>
												<source src="HW3_Vocoders/birthday.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>
										<h4>Result</h4>
										<p>
											<audio controls>
												<source src="Examples/BirthdayComb.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<h3>Daft Punk <a href = "https://pitchfork.com/news/daft-punk-call-it-quits/">Tribute</a></h3>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											y, sr = comb_tune("Tunes/daft.txt", "daft.wav", 0.25, 20)
											ipd.Audio(y, rate=sr)
										</script>
										<h4>Voice</h4>
										<p>
											<audio controls>
												<source src="HW3_Vocoders/daft.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>
										<h4>Result</h4>
										<p>
											<audio controls>
												<source src="Examples/DaftComb.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<h2><a name = "part2">Part 2: STFT-Based Vocoder</a></h2>

										<h3><a name = "istft">Inverse STFT (8 Points)</a></h3>
										<p>
											It is possible to invert the STFT by following what's known as a "shift-overlapp-add" (SOLA) procedure.  In particular, we loop through each window in the STFT, take its inverse DFT, shift the samples to the appropriate position, and add them to the values that are already there.  For example, if we have a <b>window length</b> of 1024 samples and a <b>hop length</b> of 512 samples,
											<ul>
												<li>
													The first inverted window should be added to the output at slice <b>0:1024</b></li>
												<li>
													The second inverted window should be added from <b>512:1024+512</b>
												</li>
												<li>
													The third inverted window should be added from <b>512*2:1024+512*2</b>
												</li>
												<li>
													... etc
												</li>
											</ul>
										</p>

										<p>
											We do have to be mindful of the fact that we multiplied the audio by a window before we took the STFT.  However, in the case of a <a href = "https://ursinus-cs472a-s2021.github.io/Modules/Module11/Video1">Hann window</a>, everything works out nicely when the ratio between the window length and the hop length is a power of 2, and the overlapping regions sum to a constant number.  In particular, if the ratio is <b>Q = w/h</b>, then the windows will sum to a constant <b>Q/2</b>.  For instance, when the window length is 1024 samples and the hop length is 512 samples, the windows add together as follows (where each individual window is depicted with a different color, and you notice that the windows add together where they overlap):
										</p>

										<img src = "Examples/ISTFTHann2.svg">

										<p>
											When the window length is 1024 samples and the hop length is 256 samples, the windows add together as follows
										</p>

										<img src = "Examples/ISTFTHann4.svg">

										<p>
											Therefore, you can simply divide the shifted and overlapped inverse STFT by <b>Q/2</b> to recover the original audio.
										</p>

										<h4>Your Task</h4>
										<p>
											Fill in the method <code>istft</code> to return audio samples corresponding to a complex STFT.  To invert the fourier transform in each window, you can use <code>np.real(<a href = "https://numpy.org/doc/stable/reference/generated/numpy.fft.ifft.html">np.fft.ifft</a>())</code>. (The np.real is there because due to rounding error, it's possible to have very small imaginary components upon inversion that mean nothing).  To test this, perform and STFT and see if you can recover the original audio by performing your inverse STFT.  For example, the run the following code
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr, x = wavfile.read("mj.wav")
											w = 1024
											h = 256
											S = stft(x, w, h, hann_window)
											y = istft(S, w, h, hann_window)
											ipd.Audio(y, rate=sr)
										</script>
										<p>
											Which should produce the following audio if everything is working properly:
										</p>
										<p>
											<audio controls>
												<source src="HW3_Vocoders/mj.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<h3><a name = "stftvocoder">STFT-Based Vocoder (7 Points)</a></h3>
										<p>
											Once you have the inverse STFT working, there's actually a very simple procedure we can use to make a vocoder shaped around any instrument sound.  Let's say we have instrument audio <b>I</b> and voice audio <b>V</b>.  We're going to use the voice audio to shape the instrument audio by mixing together their STFTs.  In particular, simply multiply each element in <b>STFT(I)</b> by the corresponding <u>amplitude</u> in <b>STFT(V)</b>, and then take the inverse STFT.
										</p>
										<p>
											We do have to be careful if we use the synthetic instruments from <a href = "../HW2_DigitalInstruments">assignment 2</a> for the instrument sound, as the base frequency and its integer harmonics don't occur densely enough to support the spectrum of the voice in a way that is intelligible.  To mitigate this, we have to apply some nonlinear distortion to the instrument sound to expand its frequency content so that it has enough to support the voice.  As with the <a href = "../HW2_DigitalInstruments/index.html#electricguitar">electric guitar</a>, we can do this by passing our instrument audio samples through <code>np.sign</code>, which is akin to turning an electronic amplifier way up (this is the way we get that gritty, harmonically rich electric guitar sound in rock music).
										</p>
										<h4>Your Task</h4>
										<p>
											Fill in the method <code>spectrogram_vocoder</code>.  Below is an example using the voice to shape an FM brass sound with this method
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr = 44100
											w = 2048
											h = 64
											tune = make_tune("Tunes/daft.txt", 0.25, 44100, fm_brass_note)
											tune = np.sign(tune)
											sr, voice = wavfile.read("daft.wav")
											y = specgram_vocoder(tune, voice, sr, w, h, hann_window)
											ipd.Audio(y, rate=sr)
										</script>

										<h4>Tune</h4>
										<p>
											<audio controls>
												<source src="Examples/Brass.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>
										<h4>Voice</h4>
										<p>
											<audio controls>
												<source src="HW3_Vocoders/daft.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>
										<h4>Result</h4>
										<p>
											<audio controls>
												<source src="Examples/DaftBrass.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<p>
											<img src = "Examples/STFTVocoder_DaftBrass.svg">
										</p>

										<p>
											Below is an example using the voice to shape an FM bell sound
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											w = 2048
											h = 64
											tune = make_tune("Tunes/daft.txt", 0.25, 44100, fm_bell_note)
											tune = np.sign(tune)
											sr, voice = wavfile.read("daft.wav")
											y = specgram_vocoder(tune, voice, sr, w, h, hann_window)
											ipd.Audio(y, rate=sr)
										</script>

										<h4>Tune</h4>
										<p>
											<audio controls>
												<source src="Examples/Bell.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>
										<h4>Voice</h4>
										<p>
											<audio controls>
												<source src="HW3_Vocoders/daft.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>
										<h4>Result</h4>										
										<p>
											<audio controls>
												<source src="Examples/DaftBell.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<p>
											<img src = "Examples/STFTVocoder_DaftBell.svg">
										</p>


										<h2><a name = "part3">Part 3: Phase Retrieval</a></h2>

										<p>
											In this section, you will use a fundamental algorithm for <i>phase retrieval</i>, which is the process of estimating the phase from a magnitude only STFT.  This is useful not only in applications when we're only given the amplitudes of our sinusoids, but also in applications where we want to warp sound in a way that would mess up the phase.  For example, if we want to play audio at a faster rate without changing the pitch, the phases need to be adjusted.  Conversely, if we want to move things up in frequency without changing the speed of the audio, we also have to adjust the phase.  A quick and dirty way of doing both of these is to simply throw away the phase, warp the magnitude only STFT images, and then recover phase in the new images.
										</p>

										<h2><a name = "griffinlim">The Griffin-Lim Algorithm (8 Points)</a></h2>

										<p>
											The earliest algorithm for phase retrieval was devised by Daniel Griffin and Jae Lim in 1984, in the <a href = "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.7151&rep=rep1&type=pdf">following paper</a>.  The idea is to perform an iterative procedure where the amplitudes are fixed but where the phases are slowly adjusted.  Let's say we start with a magnitude-only spectrogram <b>SAbs</b>.  Then the iterations of Griffin-Lim are as follows:
										</p>

										<ol>
											<li>Let <b>S = SAbs</b></li>
											<li>Let <b>A = stft(istft(S))</b></li>
											<li>Compute the phase of everything in <b>A</b></li>
											<li>Let <b>S</b> have the amplitudes of <b>SAbs</b> but the phases of <b>A</b>.  As a hint for this, if you have a matrix <b>P</b> with all of the phases of <b>A</b> expressed in radians, you can turn this into a matrix of complex exponentials <b>e<SUP>iP</SUP></b> with 
											<P>
												<code>np.exp(1j*P)</code>
											</P>
											which applies <b>e<SUP>iP</SUP></b> element-wise on every element in <b>P</b>. Now, the phases are expressed as complex numbers which can simply be multiplied by the amplitudes in <b>SAbs</b>.  In fact, if you say 

											<p>
												<code>np.abs(np.exp(1j*P))</code>
											</p>

											you will notice this gives you a matrix of all 1s.  This is because the magnitude of any complex number <b>e<SUP>i &theta; </SUP></b> is 1.  In other words, these numbers do not change the amplitude, only the phase.  This is exactly what we want.
											
											</li>
											<li>Repeat steps 2-4 for some specified number of iterations</li>
											<li>Perform one final istft on <b>S</b>, and return the resulting audio</li>
										</ol>

										<h4>Your Task</h4>
										<p>
											Fill in the <code>griffin_lim</code> method to implement this algorithm.  A simple sanity check you can do to make sure it's working properly is to perform an STFT, throw out all of the phase information, and try to go back.  For example, try the following code:
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											w =  2048
											h = 128
											win_fn = hann_window
											sr, x = wavfile.read("mj.wav")
											S = stft(x, w, h, win_fn)
											SAbs = np.abs(S) # Throw away all phase information
											y = griffin_lim(SAbs, w, h, win_fn, 10)
											ipd.Audio(y, rate=sr)
										</script>

										<p>
											The result should sound like this if things are working properly
										</p>

										<h4>Original Audio</h4>
										<p>
											<audio controls>
												<source src="HW3_Vocoders/mj.wav" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<h4>Phase Retrieval Result</h4>

										<p>
											<audio controls>
												<source src="Examples/MJGriffinLim.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<p>
											This is definitely not perfect!  Audio experts would say that this sounds a little bit "phase-y," meaning that the phases aren't totally correct (it almost has a metallic sound).  However, it's quite a good estimate considering how we threw away all of the phase information and started from nothing.
										</p>

										<p>
											For a slightly more interesting algorithm that runs with Griffin Lim as a subroutine, there is also code provided for you in <code>time_shift</code> that computes the magnitude spectrogram and stretches or compresses it along the time axis, followed by phase retrieval, to change the timing without changing the pitch.  If your phase retrieval code works properly, here is what you should get when stretching out the time by 1.5x
										</p>
										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											w =  2048
											h = 128
											win_fn = hann_window
											sr, voice = wavfile.read("mj.wav")
											y = time_shift(voice, 1.5, w, h, win_fn, 10)
											ipd.Audio(y, rate=int(sr))
										</script>

										<p>
											<audio controls>
												<source src="Examples/MJ_1.5Time.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<p>
											Here is an example compressing the time to 3/5 of what it was
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											w =  2048
											h = 128
											win_fn = hann_window
											sr, voice = wavfile.read("mj.wav")
											y = time_shift(voice, 0.6, w, h, win_fn, 10)
											ipd.Audio(y, rate=int(sr))
										</script>

										<p>
											<audio controls>
												<source src="Examples/MJ_0.6Time.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>



										<HR>
										<h2><a name = "soundimages">Phase Retrieval Application 1: Sound Images (7 Points)</a></h2>
										<p>
											One interesting "synesthetic" application that phase retrieval unlocks is that we can turn images into audio by taking any grayscale image and considering it a magnitude only specification of an STFT.  If we apply phase retrieval, we can then fill in the phases for a sound whose absolute value STFT would match the specified image.
										</p>

										<p>
											<b>NOTE: </b> John Muller (from spring 2021) pointed out that Aphex Twin has a song <a href = "https://en.wikipedia.org/wiki/Windowlicker#Spectrogram">where they actually do this</a>!
										</p>

										<h4>Your Task</h4>
										<p>
											Fill in the method <code>im2sound</code> to create a sound from an image.  You will have to copy over the pixels of the image to a spectrogram, as well as their <a href = "../../../Modules/Module13/Video1">mirror image</a> to ensure that the inverse DFT produces real numbers.  See some of the examples below
										</p>

										<h3>Example 1: 1-Pixel Trajectory</h3>
										<p>
											Let's suppose we have the following image, in which a single pixel is lit up and moves around the frequency axis over time.
										</p>

										<img src = "HW3_Vocoders/Trajectory.png">

										<p>
											This is a slightly easier way to specify frequencies over time than what we did before with <a href = "https://ursinus-cs472a-s2021.github.io/Modules/Module4/Video1">chirps</a>, but we have to invert it back to the time domain to hear it.  If we create the following complex magnitude spectrogram with a window length of 2048 that has zero phase everywhere
										</p>

										<img src = "Examples/TrajectorySpec.png">
										<p>
											And then invert it with phase retrieval, we get the following audio
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr = 44100
											w = 2048
											h = 512
											y = im2sound("Trajectory.png", w, h, hann_window, 10)
											ipd.Audio(y, rate=sr)
										</script>

										<p>
											<audio controls>
												<source src="Examples/Trajectory.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>

										<h3>Example 2: DOOM</h3>
										<p>
											Let's suppose we have the following image:
										</p>

										<img src = "HW3_Vocoders/Doom.png">

										<p>
											If we create the following complex magnitude spectrogram with a window length of 2048, being careful to mirror the frequencies appropriately:
										</p>

										<img src = "Examples/DoomSpec.png">
										<p>
											And then invert it with phase retrieval, we get the following audio
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr = 44100
											w = 2048
											h = 512
											y = im2sound("Doom.png", w, h, hann_window, 10)
											ipd.Audio(y, rate=sr)
										</script>

										<p>
											<audio controls>
												<source src="Examples/Doom.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>


										<h3>Example 3: CS 372</h3>
										<p>
											Let's suppose we have the following image:
										</p>

										<img src = "HW3_Vocoders/CS372.png">

										<p>
											If we create the following complex magnitude spectrogram with a window length of 2048
										</p>

										<img src = "Examples/CS372Spec.png">
										<p>
											And then invert it with phase retrieval, we get the following audio
										</p>

										<script type="syntaxhighlighter" class="brush: py"><![CDATA[
											sr = 44100
											w = 2048
											h = 512
											y = im2sound("CS472.png", w, h, hann_window, 10)
											ipd.Audio(y, rate=sr)
										</script>

										<p>
											<audio controls>
												<source src="Examples/CS372.mp3" type="audio/mpeg">
											  Your browser does not support the audio element.
											</audio> 
										</p>



										<HR>
											<h2><a name = "beepytunes">Phase Retrieval Application 2: Beepy Tunes, Aka Tricking Shazam (7 Points)</a></h2>

											<p>
												The "Shazam technique" is an algorithm for recognizing a tune from a short clip of audio in a noisy environment, and it's led to the popular <a href = "https://www.shazam.com/">shazam app</a>.  The ingenious idea of this algorithms it retain only maxes in the spectrogram, and to throw everything else away.  You can read more details about this algorithm in the <a href = "https://www.princeton.edu/~cuff/ele201/files/Wang03-shazam.pdf">original paper</a> and in a <a href = "https://ursinus-cs174-s2022.github.io/CoursePage/Assignments/FinalProject_Shazam/background.html">writeup I made</a>.
											</p>
	
											<p>
												The first step that Shazam does is find every point in the spectrogram that is greater than all of its neighbors in a rectangle around it 
											</p>
											<img src = "WindowMaxLabeled.svg">
											<p>
												Since we know this, we can make a spectrogram with only the maxes and trick the Shazam algorithm into thinking it's the original.  For example, below is a spectrogram that we get by picking out all of time/frequency points in a 20 second clip from <a href = "https://www.youtube.com/watch?v=dQw4w9WgXcQ">Rick Astley's famous tune</a> within a 17 x 11 window whose max frequency bin is 256:
											</p>

											<img src = "Examples/RickRollMaxes.png">

											<p>
												If we invert the spectrogram using Griffin-Lim, we get the following "tune":
											</p>

											<p>
												<audio controls>
													<source src="Examples/rickRollTrick.wav" type="audio/mpeg">
												  Your browser does not support the audio element.
												</audio> 
											</p>

											<p>
												If you hold up your phone with Shazam, it will be tricked into thinking this is Rick Astley.  So you can rick roll your phone!
											</p>

											<p>
												As a side note, I made a web site to do this live at <a href = "https://www.beepytunes.com">https://www.beepytunes.com</a>
											</p>


											<h4>Your Task</h4>
											<p>
												Fill in the <code>make_beepy_tune</code> to create a beepy tune from a particular audio waveform.  Below is the code you can use to replicate the Rick Roll example above:
											</p>

											<script type="syntaxhighlighter" class="brush: py"><![CDATA[
												sr, x = wavfile.read("rickroll.wav")
												w = 2048
												h = 1024
												win_fn = sin_window
												time_win = 5
												freq_win = 8
												n_iters = 3
												max_freq = 256
												y = make_beepy_tune(x, w, h, win_fn, time_win, freq_win, max_freq, n_iters)
												ipd.Audio(y, rate=sr)
											</script>

											<h4>
												Tips / Tricks
											</h4>
											<ul>
												<li>As you did with sound images, you will have to mirror the frequencies </li>
												<li>
													You can accomplish this task with a doubly-nested for loop, but have a look at the documentation for the <code><a href = "https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.maximum_filter.html">maximum_filter</a></code> method in the <code>scipy.ndimage</code> library.  One call to this method with the appropriate parameters could save you that entire loop system!
												</li>
											</ul>
											


										<HR>
											<h2><a name = "pitchshift">Phase Retrieval Application 3: Pitch Shifting (Extra Credit +3)</a></h2>
	
											<p>
												For those who want a challenge, you can try to implement pitch shifting by nonlinearly warping the frequency axis, resampling, and performing phase retrieval (though this will be ungraded).  Have a look a the <code>time_shift</code> method to see how to do the warp, and fill in the <code>pitch_shift</code> method.  Here is an example shifting down in pitch by 4 halfsteps.  Notice how the frequencies get compressed.
											</p>
											<script type="syntaxhighlighter" class="brush: py"><![CDATA[
												w =  8192
												h = 128
												win_fn = hann_window
												sr, voice = wavfile.read("mj.wav")
												y = pitch_shift(voice, -4, w, h, win_fn, 10)
												ipd.Audio(y, rate=int(sr))
											</script>
	
											<p>
												<audio controls>
													<source src="Examples/MJ_8192_128_-4.mp3" type="audio/mpeg">
												  Your browser does not support the audio element.
												</audio> 
											</p>
	
											<img src = "Examples/MJ_8192_128_-4.svg" width=1000>
											
	
											<p>
												Here is an example shifting up in pitch by 4 halfsteps.  Notice how the frequencies get stretched out.
											</p>
											<script type="syntaxhighlighter" class="brush: py"><![CDATA[
												w =  8192
												h = 128
												win_fn = hann_window
												voice, sr = wavfile.read("mj.wav")
												y = pitch_shift(voice, 4, w, h, win_fn, 10)
												ipd.Audio(y, rate=int(sr))
											</script>
	
											<p>
												<audio controls>
													<source src="Examples/MJ_8192_128_4.mp3" type="audio/mpeg">
												  Your browser does not support the audio element.
												</audio> 
											</p>
	
											<img src = "Examples/MJ_8192_128_4.svg" width=1000>

										<h2><a name = "musical">Musical Statement (3 Points)</a></h2>
										<p>
											You just made some really wacky compositional tools, so use them!  As usual, a submission is required, but you may also choose to submit your entry to a contest, where the winner will earn 2 extra credit points.
										</p>
										<p>
											If you're going to use the vocoders, you have to get the timing between the voice and the notes to line up very well.  I'd recommend you use a <a href = "https://www.google.com/search?q=metronome">metronome</a> to keep time as you're recording.  Also, you are certainly not limited to the synthetic instruments we made in homework 2!  If you play your own instrument, you can record that (you may need to distort it).  If you don't play an instrument, you can find tunes on youtube or other places online.
										</p>

										<h2><a name = "bored">For the bored...</a></h2>
										<p>
											See if you can apply a warp that changes over time and then apply phase retrieval.  For example, what if you try to add vibrato to your STFT?
										</p>





                                    
                                </div>
						</div>
					</div>

					<!--LaTeX in Javascript!-->
					<script src="../../../../jsMath/easy/load.js"></script>
					<!--Syntax highlighting in Javascript!-->
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shCore.js"></script>
					<script type="text/javascript" src="../../../syntaxhighlighter/scripts/shBrushJScript.js"></script>
                    <script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushCpp.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushXml.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushMatlabSimple.js"></script>
					<script type="text/javascript" src="../../../../syntaxhighlighter/scripts/shBrushPython.js"></script>
					<link type="text/css" rel="stylesheet" href="../../../../syntaxhighlighter/styles/shCoreDefault.css"/>
					<script type="text/javascript">SyntaxHighlighter.all();</script>

<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
                                        <li>
											<span class="opener">General</span>
											<ul>
												<li><a href = "../../index.html#overview">Overview</a></li>
												<li><a href = "../../index.html#logistics">Technology Logistics</a></li>
												<li><a href = "../../index.html#homework">Homework</a></li>
												<li><a href = "../../index.html#grading">Grading</a>
													<ul>
														<li><a href = "../../index.html#deadlines">Deadlines Policy</a></li>
													</ul>
												</li>
												<li><a href = "../../index.html#environment">Classroom Environment</a></li>
												<li><a href = "../../index.html#participation">Participation</a></li>
												<li><a href = "../../index.html#collaboration">Collaboration Policy</a></li>
												<li><a href = "../../index.html#other">Other Resources / Policies</a></li>
											</ul> 
										</li>
										<li><a href = "../../Software/index.html">Software</a></li>
										<li><a href = "../../../webaudio-pianoroll/index.html">Piano Roll Editor</a></li>
										<li><a href = "https://calebj0seph.github.io/spectro/">Live Spectrogram Viewer</a></li>
										<li><a href = "../../index.html#schedule">Schedule</a></li>
                                        <li>
											<span class="opener">Assignments</span>
											<ul>
												<li>
													<a href = "../../Assignments/HW1_RissetBeats">HW1: Risset Beats</a>
													<ul>
														<li>
															<a href = "../../Assignments/HW1_RissetBeats/statements.html">Musical Statements</a>
														</li>
													</ul>
												</li>
												<li><a href = "../../Assignments/HW2_DigitalInstruments">HW2: Digital Instruments</a>
												<!--<ul>
													<li>
														<a href = "../../Assignments/HW2_DigitalInstruments/statements.html">Musical Statements</a>
													</li>
												</ul>!-->
												</li>
												<li><a href = "../../Assignments/HW3_Vocoders">HW3: Spectacular Spectrograms</a>
												<!--<ul>
													<li>
														<a href = "../../Assignments/HW3_Vocoders/statements.html">Musical Statements</a>
													</li>
												</ul>!-->
												</li>
												<!--
												<li><a href = "../../Assignments/HW4_RhythmAnalysis">HW4: Tempo Estimation And Beat Tracking</a></li>
												<li><a href = "../../Assignments/HW5_LetItBee">HW5: Let It Bee</a>
												<ul>
													<li>
														<a href = "../../Assignments/HW5_LetItBee/statements.html">Musical Statements</a>
													</li>
												</ul>
												</li>
												<li><a href = "../../Assignments/HW6_StringAlong">HW6: String Along</a>
												<ul>
													<li>
														<a href = "../../Assignments/HW6_StringAlong/statements.html">Musical Statements</a>
													</li>
												</ul>
												
												</li>
												-->
											</ul>
										</li>
										<li>
											<span class="opener">Class Exercises</span>
											<ul>
												<li><a href = "../../ClassExercises/Week1/Week1_AudioReverseGame/">Week 1: Audio Reverse Game</a></li>
												<li><a href = "../../ClassExercises/Week2/Week2_BeatPhase/index.html">Week 2: Beat Phase</a></li>
												<li><a href = "../../ClassExercises/RissetNotes/index.html">Week 2: Notes on Risset Beats</a></li>
												<li><a href = "../../ClassExercises/Week2/Week2_Harmonicity/index.html">Week 2: Harmonicity</a></li>
												<li><a href = "../../ClassExercises/Week3/Week3_ZCS_Loudness/index.html">Week 3: Zero Crossings And Loudness Perception</a></li>
												<li><a href = "../../ClassExercises/Week3/Week3_Timbre/index.html">Week 3: Harmonics And Timbre</a></li>
												<li><a href = "../../ClassExercises/Week4/Week4_Envelopes/index.html">Week 4: Timbral Envelopes</a></li>
												<li>
													<a href = "../../ClassExercises/Week4/Week4_CombFilters/index.html">Week 4: Comb Filters</a>
												</li>
												<li><a href = "../../ClassExercises/Week5/Week5_DFT/index.html">Week 5: The Discrete Fourier Transform</a></li>
												<li><a href = "../../ClassExercises/Week6/2DArrays/index.html">Week 6: 2D Arrays And Spectrograms</a></li>
												<li><a href = "../../ClassExercises/Week6/ComplexDFT/index.html">Week 6: Complex DFT</a></li>
												<li><a href = "../../ClassExercises/Week7/Week7_STFTNoiseShaping">Week 7: STFT Noise Shaping</a></li>
												<li><a href = "../../ClassExercises/Week8/Week8_ANF">Week 8: Audio Novelty Functions</a></li>
												<li><a href = "../../ClassExercises/BeattrackNotes">Week 9: Notes on Dynamic Programming Beat Tracking</a></li>
												<li><a href = "../../ClassExercises/Week9/Week9_MFCC">Week 9: Mel-Frequency Cepstral Coefficients (MFCCs)</a></li>
												<li><a href = "https://github.com/ursinus-cs372-s2023/pyshazam">Week 9: Python Implementation of Shazam</a></li>
												<li><a href = "https://github.com/ursinus-cs372-s2023/Week10_HPSS/tree/classcode">Week 10: Harmonic/Percussive Source Separation with Median Filters</a></li>
												<li><a href = "../../ClassExercises/Week10/Week10_NMF">Week 10: Nonnegative Matrix Factorization for Demixing</a></li>
												<li><a href = "../../ClassExercises/Week11/">Week 11: Fundamental Frequency Tracking And Autotuners</a></li>
												<li><a href = "../../../Modules/Module21/Video0">Week 12: Linear Separability of Phase-Shifted Triangle/Square Waves</a></li>
											</ul>
										</li>
                                        <li>
											<span class="opener">Pre-Class Modules</span>
											<ul>
												<li><a href = "../../../Modules/Module1/Video0">Module 1: Digital Audio Waveforms, Python Basics</a></a></li>
												<li><a href = "../../../Modules/Module2/Video1">Module 2: Sinusoids And Simple Numpy Tunes</a></li>
												<li><a href = "../../../Modules/Module3/Video0">Module 3: Standing Waves And Plucked String Synthesis</a></li>
												<li><a href = "../../../Modules/Module4/Video1">Module 4: Chirps, Instantaneous Frequency, Vibrato, Sonification</a></li>
												<li><a href = "../../../Modules/Module5/Video1">Module 5: Zero Crossings Filtering, Loudness And Intensity / Dynamics</a></li>
												<li><a href = "../../../Modules/Module6/Video0">Module 6: Timbre, FM Synthesis, Python Methods As Parameters</a></li>
												<li><a href = "../../../Modules/Module7/Video1">Module 7: Echoes, Impulse Responses, And Convolution</a></li>
												<li><a href = "../../../Modules/Module8/Video1">Module 8: Discovering The Discrete Fourier Transform</a></li>
												<li><a href = "../../../Modules/Module9/Video0">Module 9: The Real Discrete Fourier Transform (DFT), Amplitude/Phase</a></li>
												<li><a href = "../../../Modules/Module10/Video1">Module 10: DFT on Real Audio, DFT on Sawtooth/Square Waves, Fundamental DFT Properties, Inverse DFT And Fast Risset</a></li>
												<li><a href = "../../../Modules/Module11/Video1">Module 11: STFT, Window Functions, Complex Numbers</a></li>
												<li><a href = "../../../Modules/Module12/Video1">Module 12: Complex DFT And Phasors</a></li>
												<li><a href = "../../../Modules/Module13/Video1.html">Module 13: Aliasing, Inverse DFT</a></li>
												<li><a href = "../../../Modules/Module14/Video1">Module 14: Convolution And Multiplication Duality</a></li>
												<li><a href = "../../../Modules/Module15/Video0">Module 15: The Z Transform</a></li>
												<li><a href = "../../../Modules/Module16/Video0">Module 16: Audio Novelty Functions, Tempo Estimation, Matrix Multiplication</a></li>
												<li><a href = "../../../Modules/Module17/Video0">Module 17: Sonifying Mel And Chroma Filterbanks</a></li>
												<li><a href = "../../../Modules/Module18/Video0">Module 18: Matrix Multiplication for Audio Activations</a></li>
												<li><a href = "../../../Modules/Module19/Video0">Module 19: Self-Similarity Matrices</a></li>
												<li><a href = "../../../Modules/Module20/Video1">Module 20: Intro To Supervised Learning, Logistic Regression, Gradient Descent, And PyTorch</a></li>
												<li><a href = "../../../Modules/Module21/Video0">Module 21: Neural Networks</a></li>
												<li><a href = "../../../Modules/Module22/Video1">Module 22: Multiclass Classification, Convolutional Neural Networks, And Overfitting</a></li>
											</ul>
										</li>
										<li><a href = "https://docs.google.com/forms/d/e/1FAIpQLSfwkO_w_Ku-n2Ou6J7pF--i0C2-a20Ov9wf690T6cYx80ASsw/viewform?usp=sf_link">Anonymous Question</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<div class="mini-posts">
										Announcements							
                                    </div>
								</section>


							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; <a href = "http://www.ctralie.com">Christopher J. Tralie</a>. All rights reserved.  Contact chris.tralie@gmail.com. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
			
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- End Sidebar !-->

<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
<!-- End Scripts -->
	</body>